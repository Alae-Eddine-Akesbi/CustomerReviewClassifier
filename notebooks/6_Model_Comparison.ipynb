{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Comparaison des Performances des Modèles\n",
    "\n",
    "## 6.1. Introduction\n",
    "\n",
    "Ce notebook finalise le projet en comparant les performances de tous les modèles entraînés sur un jeu de données de test commun. Cela nous permet d'évaluer objectivement quelle approche est la plus efficace pour cette tâche de classification de sentiments.\n",
    "\n",
    "**Modèles Comparés :**\n",
    "- Régression Logistique\n",
    "- Support Vector Machine (SVM)\n",
    "- BERT (fine-tuné avec LoRA)\n",
    "- XLNet (fine-tuné)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Installation et Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers scikit-learn pandas joblib matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, XLNetForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Chargement de tous les modèles et pré-requis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Utilisation du device : {device}\")\n",
    "\n",
    "# Charger les modèles classiques\n",
    "lr_model = joblib.load('../models/logistic_regression/logreg_model.pkl')\n",
    "lr_vectorizer = joblib.load('../models/logistic_regression/tfidf_vectorizer.pkl')\n",
    "label_encoder = joblib.load('../models/logistic_regression/label_encoder.pkl')\n",
    "\n",
    "svm_model = joblib.load('../models/svm/svm_model.pkl')\n",
    "svm_vectorizer = joblib.load('../models/svm/tfidf_vectorizer.pkl')\n",
    "\n",
    "# Charger les modèles Transformer\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"../models/bert_lora\").to(device)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"../models/bert_lora\")\n",
    "\n",
    "xlnet_model = XLNetForSequenceClassification.from_pretrained(\"../models/xlnet\").to(device)\n",
    "xlnet_tokenizer = AutoTokenizer.from_pretrained(\"../models/xlnet\")\n",
    "\n",
    "print(\"\\nTous les modèles ont été chargés avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4. Préparation du Jeu de Données de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('../data/balanced_subset.csv')\n",
    "test_sample = full_data.sample(n=10000, random_state=42)\n",
    "X_test_text = test_sample['cleaned_text'].dropna().tolist()\n",
    "y_test_true_labels = test_sample.loc[test_sample['cleaned_text'].notna(), 'rating'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5. Exécution des Prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_transformer(texts, model, tokenizer, batch_size=32):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        predictions.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Prédictions des modèles classiques\n",
    "lr_preds_labels = label_encoder.inverse_transform(lr_model.predict(lr_vectorizer.transform(X_test_text)))\n",
    "svm_preds_labels = svm_model.predict(svm_vectorizer.transform(X_test_text))\n",
    "\n",
    "# Prédictions des modèles Transformer\n",
    "bert_preds_encoded = predict_transformer(X_test_text, bert_model, bert_tokenizer)\n",
    "xlnet_preds_encoded = predict_transformer(X_test_text, xlnet_model, xlnet_tokenizer)\n",
    "\n",
    "bert_preds_labels = label_encoder.inverse_transform(bert_preds_encoded)\n",
    "xlnet_preds_labels = label_encoder.inverse_transform(xlnet_preds_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6. Rapports de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Performance de la Régression Logistique ---\")\n",
    "print(classification_report(y_test_true_labels, lr_preds_labels))\n",
    "\n",
    "print(\"--- Performance du SVM ---\")\n",
    "print(classification_report(y_test_true_labels, svm_preds_labels))\n",
    "\n",
    "print(\"--- Performance de BERT ---\")\n",
    "print(classification_report(y_test_true_labels, bert_preds_labels))\n",
    "\n",
    "print(\"--- Performance de XLNet ---\")\n",
    "print(classification_report(y_test_true_labels, xlnet_preds_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7. Visualisation de la Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['Logistic Regression', 'SVM', 'BERT', 'XLNet']\n",
    "accuracies = [\n",
    "    accuracy_score(y_test_true_labels, lr_preds_labels),\n",
    "    accuracy_score(y_test_true_labels, svm_preds_labels),\n",
    "    accuracy_score(y_test_true_labels, bert_preds_labels),\n",
    "    accuracy_score(y_test_true_labels, xlnet_preds_labels)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=model_names, y=accuracies, palette='viridis')\n",
    "plt.title('Comparaison de la Précision des Modèles')\n",
    "plt.ylabel('Précision (Accuracy)')\n",
    "plt.ylim(0.8, 0.95) # Ajuster les limites pour une meilleure visualisation\n",
    "\n",
    "for index, value in enumerate(accuracies):\n",
    "    plt.text(index, value + 0.005, f'{value:.3f}', ha='center', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=label_encoder.classes_)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "    plt.title(f'Matrice de Confusion - {model_name}')\n",
    "    plt.xlabel('Prédit')\n",
    "    plt.ylabel('Vrai')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test_true_labels, lr_preds_labels, 'Régression Logistique')\n",
    "plot_confusion_matrix(y_test_true_labels, svm_preds_labels, 'SVM')\n",
    "plot_confusion_matrix(y_test_true_labels, bert_preds_labels, 'BERT')\n",
    "plot_confusion_matrix(y_test_true_labels, xlnet_preds_labels, 'XLNet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
